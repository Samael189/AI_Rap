{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Skyzoo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samael189/AI_Rap/blob/master/AI_Skyzoo_v3(rhyming).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oi7NXc1CnYb",
        "colab_type": "code",
        "outputId": "9a99ffcc-d22a-4df7-a8c0-a3ae784f5dbb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "uploaded = files.upload()\n",
        "all_files = [(name,os.path.getmtime(name)) for name in os.listdir()]\n",
        "latest_file = sorted(all_files,key=lambda x: -x[1])[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90e9cdce-d189-46ef-abcc-14e35adfac42\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-90e9cdce-d189-46ef-abcc-14e35adfac42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Skyzoo.txt to Skyzoo.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM2_LNOWBoYU",
        "colab_type": "code",
        "outputId": "f94681cc-cba5-4f7e-b450-8362d0618562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Dependencies\n",
        "# just installing different packages that are needed.\n",
        "# markovify handles markov chains\n",
        "# pronouncing handles rhymes\n",
        "!pip install markovify\n",
        "!pip install pronouncing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting markovify\n",
            "  Downloading https://files.pythonhosted.org/packages/99/b7/a5cf39283f08c8013623dbcf67063b0215942ae464fc864eca1434d050e1/markovify-0.7.2.tar.gz\n",
            "Collecting unidecode (from markovify)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/19/38/b901adb8ab0721a6c8c86f468e48b22f3ecf08560e6aeb99fa\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.7.2 unidecode-1.1.1\n",
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0 (from pronouncing)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/bc/606843d7cfe4d82f5a21fc46d1ae8e364ac20c57e68d1ec4190bce4f2734/cmudict-0.4.2-py2.py3-none-any.whl (938kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 5.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.2 pronouncing-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I01g0IYYE-0I",
        "colab_type": "code",
        "outputId": "b64dc8c8-edad-4e7f-b6d8-547d82c98db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import markovify\n",
        "import re\n",
        "import pronouncing\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJTh2AkCBed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are just the parameters of the network.\n",
        "depth = 4  # depth of the network. changing will require a retrain\n",
        "maxsyllables = 16  # maximum syllables per line. Change this freely without retraining the network\n",
        "rap_length = 80 # number of lines in the rap song\n",
        "epochs_to_train = 30 # how many times the network trains on the whole dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW0QNXLpMk6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rap_source = \"Skyzoo.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlfQxJucFXIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(depth):\n",
        "    # Sequential() creates a linear stack of layers\n",
        "    model = Sequential()\n",
        "    # Adds a LSTM layer as the first layer in the network with\n",
        "    # 4 units (nodes), and a 2x2 tensor (which is the same shape as the\n",
        "    # training data)\n",
        "    model.add(LSTM(4, input_shape=(2, 2), return_sequences=True))\n",
        "    # adds 'depth' number of layers to the network with 8 nodes each\n",
        "    for i in range(depth):\n",
        "        model.add(LSTM(8, return_sequences=True))\n",
        "    # adds a final layer with 2 nodes for the output\n",
        "    model.add(LSTM(2, return_sequences=True))\n",
        "    # prints a summary representation of the model\n",
        "    model.summary()\n",
        "    # configures the learning process for the network / model\n",
        "    # the optimizer function rmsprop: optimizes the gradient descent\n",
        "    # the loss function: mse: will use the \"mean_squared_error when trying to improve\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='mse')\n",
        "\n",
        "    #if artist + \".rap\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    # loads the weights from the hdf5 file saved earlier\n",
        "    #    model.load_weights(str(artist + \".rap\"))\n",
        "    #    print \"loading saved network: \" + str(artist) + \".rap\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def markov(text_file):\n",
        "    read = rap_source\n",
        "    # markovify goes line by line of the lyrics.txt file and\n",
        "    # creates a model of the text which allows us to use\n",
        "    # make_sentence() later on to create a bar for lyrics\n",
        "    # creates a probability distribution for all the words\n",
        "    # so it can generate words based on the current word we're on\n",
        "    text_model = markovify.NewlineText(read)\n",
        "    return text_model\n",
        "\n",
        "\n",
        "# used when generating bars and making sure the length is not longer\n",
        "# than the max syllables, and will continue to generate bars until\n",
        "# the amount of syllables is less than the max syllables\n",
        "def syllables(line):\n",
        "    count = 0\n",
        "    for word in line.split(\" \"):\n",
        "        vowels = 'aeiouy'\n",
        "        word = word.lower().strip(\".:;?!\")\n",
        "        if word[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word)):\n",
        "            if word[index] in vowels and word[index - 1] not in vowels:\n",
        "                count += 1\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if word.endswith('le'):\n",
        "            count += 1\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "    return count / maxsyllables\n",
        "\n",
        "\n",
        "# writes a rhyme list to a rhymes file that allows for use when\n",
        "# building the dataset, and composing the rap\n",
        "def rhymeindex(lyrics):\n",
        "    #if str(artist) + \".rhymes\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    print \"loading saved rhymes from \" + str(artist) + \".rhymes\"\n",
        "    #    return open(str(artist) + \".rhymes\", \"r\").read().split(\"\\n\")\n",
        "    if True:\n",
        "        rhyme_master_list = []\n",
        "        print (\"Alright, building the list of all the rhymes\")\n",
        "        for i in lyrics:\n",
        "            # grabs the last word in each bar\n",
        "            word = re.sub(r\"\\W+\", '', i.split(\" \")[-1]).lower()\n",
        "            # pronouncing.rhymes gives us a word that rhymes with the word being passed in\n",
        "            rhymeslist = pronouncing.rhymes(word)\n",
        "            # need to convert the unicode rhyme words to UTF8\n",
        "            rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "            # rhymeslistends contains the last two characters for each word\n",
        "            # that could potentially rhyme with our word\n",
        "            rhymeslistends = []\n",
        "            for i in rhymeslist:\n",
        "                rhymeslistends.append(i[-2:])\n",
        "            try:\n",
        "                # rhymescheme gets all the unique two letter endings and then\n",
        "                # finds the one that occurs the most\n",
        "                rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "            except Exception:\n",
        "                rhymescheme = word[-2:]\n",
        "            rhyme_master_list.append(rhymescheme)\n",
        "        # rhyme_master_list is a list of the two letters endings that appear\n",
        "        # the most in the rhyme list for the word\n",
        "        rhyme_master_list = list(set(rhyme_master_list))\n",
        "\n",
        "        reverselist = [x[::-1] for x in rhyme_master_list]\n",
        "        reverselist = sorted(reverselist)\n",
        "        # rhymelist is a list of the two letter endings (reversed)\n",
        "        # the reason the letters are reversed and sorted is so\n",
        "        # if the network messes up a little bit and doesn't return quite\n",
        "        # the right values, it can often lead to picking the rhyme ending next to the\n",
        "        # expected one in the list. But now the endings will be sorted and close together\n",
        "        # so if the network messes up, that's alright and as long as it's just close to the\n",
        "        # correct rhymes\n",
        "        rhymelist = [x[::-1] for x in reverselist]\n",
        "\n",
        "        #f = open(str(artist) + \".rhymes\", \"w\")\n",
        "        #f.write(\"\\n\".join(rhymelist))\n",
        "        #f.close()\n",
        "        print (rhymelist)\n",
        "        return rhymelist\n",
        "\n",
        "\n",
        "# converts the index of the most common rhyme ending\n",
        "# into a float\n",
        "def rhyme(line, rhyme_list):\n",
        "    word = re.sub(r\"\\W+\", '', line.split(\" \")[-1]).lower()\n",
        "    rhymeslist = pronouncing.rhymes(word)\n",
        "    rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "    rhymeslistends = []\n",
        "    for i in rhymeslist:\n",
        "        rhymeslistends.append(i[-2:])\n",
        "    try:\n",
        "        rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "    except Exception:\n",
        "        rhymescheme = word[-2:]\n",
        "    try:\n",
        "        float_rhyme = rhyme_list.index(rhymescheme)\n",
        "        float_rhyme = float_rhyme / float(len(rhyme_list))\n",
        "        return float_rhyme\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# grabs each line of the lyrics file and puts them\n",
        "# in their own index of a list, and then removes any empty lines\n",
        "# from the lyrics file and returns the list as bars\n",
        "def split_lyrics_file(text):\n",
        "    #text = open(text_file).read()\n",
        "    #text = text.split(\"\\n\")\n",
        "    while \"\" in text:\n",
        "        text.remove(\"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# only ran when not training\n",
        "def generate_lyrics(lyrics_file):\n",
        "    bars = []\n",
        "    last_words = []\n",
        "    lyriclength = len(lyrics_file)\n",
        "    count = 0\n",
        "    markov_model = markov((\". \").join(lyrics_file) + \".\")\n",
        "\n",
        "    while len(bars) < lyriclength / 9 and count < lyriclength * 2:\n",
        "        # By default, the make_sentence method tries, a maximum of 10 times per invocation,\n",
        "        # to make a sentence that doesn't overlap too much with the original text.\n",
        "        # If it is successful, the method returns the sentence as a string.\n",
        "        # If not, it returns None. (https://github.com/jsvine/markovify)\n",
        "        bar = markov_model.make_sentence()\n",
        "\n",
        "        # make sure the bar isn't 'None' and that the amount of\n",
        "        # syllables is under the max syllables\n",
        "        if type(bar) != type(None) and syllables(bar) < 1:\n",
        "\n",
        "            # function to get the last word of the bar\n",
        "            def get_last_word(bar):\n",
        "                last_word = bar.split(\" \")[-1]\n",
        "                # if the last word is punctuation, get the word before it\n",
        "                if last_word[-1] in \"!.?,\":\n",
        "                    last_word = last_word[:-1]\n",
        "                return last_word\n",
        "\n",
        "            last_word = get_last_word(bar)\n",
        "            # only use the bar if it is unique and the last_word\n",
        "            # has only been seen less than 3 times\n",
        "            if bar not in bars and last_words.count(last_word) < 3:\n",
        "                bars.append(bar)\n",
        "                last_words.append(last_word)\n",
        "                count += 1\n",
        "\n",
        "    return bars\n",
        "\n",
        "\n",
        "# used to construct the 2x2 inputs for the LSTMs\n",
        "# the lyrics being passed in are lyrics (original lyrics if being trained,\n",
        "# or ours if it's already trained)\n",
        "def build_dataset(lyrics, rhyme_list):\n",
        "    dataset = []\n",
        "    line_list = []\n",
        "    # line_list becomes a list of the line from the lyrics, the syllables for that line (either 0 or 1 since\n",
        "    # syllables uses integer division by maxsyllables (16)), and then rhyme returns the most common word\n",
        "    # endings of the words that could rhyme with the last word of line\n",
        "    for line in lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # using range(len(dataset)) - 3 because of the way the indices are accessed to\n",
        "    # get the lines\n",
        "    for i in range(len(dataset) - 3):\n",
        "        line1 = dataset[i][1:]\n",
        "        line2 = dataset[i + 1][1:]\n",
        "        line3 = dataset[i + 2][1:]\n",
        "        line4 = dataset[i + 3][1:]\n",
        "\n",
        "        # populate the training data\n",
        "        # grabs the syllables and rhyme index here\n",
        "        x = [line1[0], line1[1], line2[0], line2[1]]\n",
        "        x = np.array(x)\n",
        "        # the data is shaped as a 2x2 array where each row is a\n",
        "        # [syllable, rhyme_index] pair\n",
        "        x = x.reshape(2, 2)\n",
        "        x_data.append(x)\n",
        "\n",
        "        # populate the target data\n",
        "        y = [line3[0], line3[1], line4[0], line4[1]]\n",
        "        y = np.array(y)\n",
        "        y = y.reshape(2, 2)\n",
        "        y_data.append(y)\n",
        "\n",
        "    # returns the 2x2 arrays as datasets\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    # print \"x shape \" + str(x_data.shape)\n",
        "    # print \"y shape \" + str(y_data.shape)\n",
        "    return x_data, y_data\n",
        "\n",
        "# only used when not training\n",
        "def compose_rap(lines, rhyme_list, lyrics_file, model):\n",
        "    rap_vectors = []\n",
        "    human_lyrics = split_lyrics_file(lyrics_file)\n",
        "\n",
        "    # choose a random line to start in from given lyrics\n",
        "    initial_index = random.choice(range(len(human_lyrics) - 1))\n",
        "    # create an initial_lines list consisting of 2 lines\n",
        "    initial_lines = human_lyrics[initial_index:initial_index + 8]\n",
        "\n",
        "    starting_input = []\n",
        "    for line in initial_lines:\n",
        "        # appends a [syllable, rhyme_index] pair to starting_input\n",
        "        starting_input.append([syllables(line), rhyme(line, rhyme_list)])\n",
        "\n",
        "    # predict generates output predictions for the given samples\n",
        "    # it's reshaped as a (1, 2, 2) so that the model can predict each\n",
        "    # 2x2 matrix of [syllable, rhyme_index] pairs\n",
        "    starting_vectors = model.predict(np.array([starting_input]).flatten().reshape(4, 2, 2))\n",
        "    rap_vectors.append(starting_vectors)\n",
        "\n",
        "    for i in range(rap_length):\n",
        "        rap_vectors.append(model.predict(np.array([rap_vectors[-1]]).flatten().reshape(4, 2, 2)))\n",
        "\n",
        "    return rap_vectors\n",
        "\n",
        "\n",
        "def vectors_into_song(vectors, generated_lyrics, rhyme_list):\n",
        "    print (\"\\n\\n\")\n",
        "    print (\"About to write rap (this could take a moment)...\")\n",
        "    print (\"\\n\\n\")\n",
        "\n",
        "    # compare the last words to see if they are the same, if they are\n",
        "    # increment a penalty variable which grants penalty points for being\n",
        "    # uncreative\n",
        "    def last_word_compare(rap, line2):\n",
        "        penalty = 0\n",
        "        for line1 in rap:\n",
        "            word1 = line1.split(\" \")[-1]\n",
        "            word2 = line2.split(\" \")[-1]\n",
        "\n",
        "            # remove any punctuation from the words\n",
        "            while word1[-1] in \"?!,. \":\n",
        "                word1 = word1[:-1]\n",
        "\n",
        "            while word2[-1] in \"?!,. \":\n",
        "                word2 = word2[:-1]\n",
        "\n",
        "            if word1 == word2:\n",
        "                penalty += 0.2\n",
        "\n",
        "        return penalty\n",
        "\n",
        "    # vector_half is a single [syllable, rhyme_index] pair\n",
        "    # returns a score rating for a given line\n",
        "    def calculate_score(vector_half, syllables, rhyme, penalty):\n",
        "        desired_syllables = vector_half[0]\n",
        "        desired_rhyme = vector_half[1]\n",
        "        # desired_syllables is the number of syllables we want\n",
        "        desired_syllables = desired_syllables * maxsyllables\n",
        "        # desired rhyme is the index of the rhyme we want\n",
        "        desired_rhyme = desired_rhyme * len(rhyme_list)\n",
        "\n",
        "        # generate a score by subtracting from 1 the sum of the difference between\n",
        "        # predicted syllables and generated syllables and the difference between\n",
        "        # the predicted rhyme and generated rhyme and then subtract the penalty\n",
        "        score = 1.0 - (abs((float(desired_syllables) - float(syllables))) + abs(\n",
        "            (float(desired_rhyme) - float(rhyme)))) - penalty\n",
        "\n",
        "        return score\n",
        "\n",
        "    # generated a list of all the lines from generated_lyrics with their\n",
        "    # line, syllables, and rhyme float value\n",
        "    dataset = []\n",
        "    for line in generated_lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    rap = []\n",
        "\n",
        "    vector_halves = []\n",
        "    for vector in vectors:\n",
        "        # vectors are the 2x2 rap_vectors (predicted bars) generated by compose_rap()\n",
        "        # separate every vector into a half (essentially one bar) where each\n",
        "        # has a pair of [syllables, rhyme_index]\n",
        "        vector_halves.append(list(vector[0][0]))\n",
        "        vector_halves.append(list(vector[0][1]))\n",
        "\n",
        "    for vector in vector_halves:\n",
        "        # Each vector (predicted bars) is scored against every generated bar ('item' below)\n",
        "        # to find the generated bar that best matches (highest score) the vector predicted\n",
        "        # by the model. This bar is then added to the final rap and also removed from the\n",
        "        # generated lyrics (dataset) so that we don't get duplicate lines in the final rap.\n",
        "        scorelist = []\n",
        "        for item in dataset:\n",
        "            # item is one of the generated bars from the Markov model\n",
        "            line = item[0]\n",
        "\n",
        "            if len(rap) != 0:\n",
        "                penalty = last_word_compare(rap, line)\n",
        "            else:\n",
        "                penalty = 0\n",
        "            # calculate the score of the current line\n",
        "            total_score = calculate_score(vector, item[1], item[2], penalty)\n",
        "            score_entry = [line, total_score]\n",
        "            # add the score of the current line to a scorelist\n",
        "            scorelist.append(score_entry)\n",
        "\n",
        "        fixed_score_list = []\n",
        "        for score in scorelist:\n",
        "            fixed_score_list.append(float(score[1]))\n",
        "        # get the line with the max valued score from the fixed_score_list\n",
        "        max_score = max(fixed_score_list)\n",
        "        for item in scorelist:\n",
        "            if item[1] == max_score:\n",
        "                # append item[0] (the line) to the rap\n",
        "                rap.append(item[0])\n",
        "                print (str(item[0]))\n",
        "\n",
        "                # remove the line we added to the rap so\n",
        "                # it doesn't get chosen again\n",
        "                for i in dataset:\n",
        "                    if item[0] == i[0]:\n",
        "                        dataset.remove(i)\n",
        "                        break\n",
        "                break\n",
        "    return rap\n",
        "\n",
        "\n",
        "def train(x_data, y_data, model):\n",
        "    # fit is used to train the model for 5 'epochs' (iterations) where\n",
        "    # the x_data is the training data, and the y_data is the target data\n",
        "    # x is the training and y is the target data\n",
        "    # batch_size is a subset of the training data (2 in this case)\n",
        "    # verbose simply shows a progress bar\n",
        "    model.fit(np.array(x_data), np.array(y_data),\n",
        "              batch_size=4,\n",
        "              epochs=epochs_to_train,\n",
        "              verbose=1)\n",
        "    # save_weights saves the best weights from training to a hdf5 file\n",
        "    #model.save_weights(artist + \".rap\")\n",
        "\n",
        "\n",
        "def main(depth):\n",
        "    train_mode = True\n",
        "    model = create_network(depth)\n",
        "    # change the lyrics file to the file with the lyrics you want to be trained on\n",
        "    \n",
        "    text_file = rap_source\n",
        "    \n",
        "    bars = split_lyrics_file(text_file)\n",
        "\n",
        "\n",
        "\n",
        "    rhyme_list = rhymeindex(bars)\n",
        "    \n",
        "    x_data, y_data = build_dataset(bars, rhyme_list)\n",
        "    train(x_data, y_data, model)\n",
        "    \n",
        "    bars = generate_lyrics(text_file)\n",
        "    \n",
        "    vectors = compose_rap(bars, rhyme_list, text_file, model)\n",
        "    rap = vectors_into_song(vectors, bars, rhyme_list)\n",
        "    \n",
        "    for bar in rap:\n",
        "        print (bar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqxpMgbIMYpW",
        "colab_type": "code",
        "outputId": "539e9da6-ce0d-485c-b7dd-6aca545c1938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "main(depth) # run this to get the actual rap"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 2, 4)              112       \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 2, 8)              416       \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 2, 2)              88        \n",
            "=================================================================\n",
            "Total params: 2,248\n",
            "Trainable params: 2,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e316dd91dea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run this to get the actual rap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-1de14914c798>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(depth)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrap_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mbars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_lyrics_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1de14914c798>\u001b[0m in \u001b[0;36msplit_lyrics_file\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m#text = text.split(\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'remove'"
          ]
        }
      ]
    }
  ]
}