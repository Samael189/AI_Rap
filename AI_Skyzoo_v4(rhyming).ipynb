{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kanye_network",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samael189/AI_Rap/blob/master/AI_Skyzoo_v4(rhyming).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7c1rJ8FhDb",
        "colab_type": "text"
      },
      "source": [
        "1. Run this chunk of code first - it gets your dataset ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haowk4QumdIy",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "3a7bd6e1-9762-40dc-e2f5-78be97db02a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Dataset\n",
        "import urllib2\n",
        "# kanye lyrics: https://pastebin.com/raw/9S5u08EU\n",
        "\n",
        "target_url = \"https://pastebin.com/raw/9S5u08EU\" # go to pastebin, paste in your rap lyrics, submit that and then click \"raw\", or swap it out for one of the URL's above\n",
        "dirty_rap_source = urllib2.urlopen(target_url).read()\n",
        "rap_source = [x.split(\"\\r\")[0] for x in dirty_rap_source.split(\"\\n\")]\n",
        "while \"\" in rap_source:\n",
        "  rap_source.remove(\"\")\n",
        "while \" \" in rap_source:\n",
        "  rap_source.remove(\" \")\n",
        "print \"Your dataset's first 3 rap lines: \"\n",
        "print rap_source[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your dataset's first 3 rap lines: \n",
            "['Let the suicide doors up', 'I threw suicides on the tour bus', 'I threw suicides on the private jet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q11ztMRFswC",
        "colab_type": "text"
      },
      "source": [
        "2. Run this chunk of code next - it installs all of the dependencies that you need to run the rap network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqTBepLhs_b",
        "colab_type": "code",
        "outputId": "db5b6417-b4e2-449b-b346-0463b55f36f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Dependencies\n",
        "# just installing different packages that are needed.\n",
        "# markovify handles markov chains\n",
        "# pronouncing handles rhymes\n",
        "!pip install markovify\n",
        "!pip install pronouncing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: markovify in /usr/local/lib/python2.7/dist-packages (0.7.1)\r\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python2.7/dist-packages (from markovify) (1.0.22)\n",
            "Requirement already satisfied: pronouncing in /usr/local/lib/python2.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from pronouncing) (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL70UxFeFxkZ",
        "colab_type": "text"
      },
      "source": [
        "3. Run this chunk of code third to import all of the different libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKOo5txh1qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import markovify\n",
        "import re\n",
        "import pronouncing\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YbGLxf0F87b",
        "colab_type": "text"
      },
      "source": [
        "4. Run this chunk of code fourth - you can play around with the different values here, but don't make the depth too big, unless you have a lot of time to watch the network train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GrMPaYCkGfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are just the parameters of the network.\n",
        "depth = 4  # depth of the network. changing will require a retrain\n",
        "maxsyllables = 16  # maximum syllables per line. Change this freely without retraining the network\n",
        "rap_length = 50 # number of lines in the rap song\n",
        "epochs_to_train = 3 # how many times the network trains on the whole dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZnxNaaGGpl",
        "colab_type": "text"
      },
      "source": [
        "This huge chunk of code just defines all of the functions used. Run it and scroll all the way down to generate a rap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4q26wKyg-Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(depth):\n",
        "    # Sequential() creates a linear stack of layers\n",
        "    model = Sequential()\n",
        "    # Adds a LSTM layer as the first layer in the network with\n",
        "    # 4 units (nodes), and a 2x2 tensor (which is the same shape as the\n",
        "    # training data)\n",
        "    model.add(LSTM(4, input_shape=(2, 2), return_sequences=True))\n",
        "    # adds 'depth' number of layers to the network with 8 nodes each\n",
        "    for i in range(depth):\n",
        "        model.add(LSTM(8, return_sequences=True))\n",
        "    # adds a final layer with 2 nodes for the output\n",
        "    model.add(LSTM(2, return_sequences=True))\n",
        "    # prints a summary representation of the model\n",
        "    model.summary()\n",
        "    # configures the learning process for the network / model\n",
        "    # the optimizer function rmsprop: optimizes the gradient descent\n",
        "    # the loss function: mse: will use the \"mean_squared_error when trying to improve\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='mse')\n",
        "\n",
        "    #if artist + \".rap\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    # loads the weights from the hdf5 file saved earlier\n",
        "    #    model.load_weights(str(artist + \".rap\"))\n",
        "    #    print \"loading saved network: \" + str(artist) + \".rap\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def markov(text_file):\n",
        "    read = rap_source\n",
        "    # markovify goes line by line of the lyrics.txt file and\n",
        "    # creates a model of the text which allows us to use\n",
        "    # make_sentence() later on to create a bar for lyrics\n",
        "    # creates a probability distribution for all the words\n",
        "    # so it can generate words based on the current word we're on\n",
        "    text_model = markovify.NewlineText(read)\n",
        "    return text_model\n",
        "\n",
        "\n",
        "# used when generating bars and making sure the length is not longer\n",
        "# than the max syllables, and will continue to generate bars until\n",
        "# the amount of syllables is less than the max syllables\n",
        "def syllables(line):\n",
        "    count = 0\n",
        "    for word in line.split(\" \"):\n",
        "        vowels = 'aeiouy'\n",
        "        word = word.lower().strip(\".:;?!\")\n",
        "        if word[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word)):\n",
        "            if word[index] in vowels and word[index - 1] not in vowels:\n",
        "                count += 1\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if word.endswith('le'):\n",
        "            count += 1\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "    return count / maxsyllables\n",
        "\n",
        "\n",
        "# writes a rhyme list to a rhymes file that allows for use when\n",
        "# building the dataset, and composing the rap\n",
        "def rhymeindex(lyrics):\n",
        "    #if str(artist) + \".rhymes\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    print \"loading saved rhymes from \" + str(artist) + \".rhymes\"\n",
        "    #    return open(str(artist) + \".rhymes\", \"r\").read().split(\"\\n\")\n",
        "    if True:\n",
        "        rhyme_master_list = []\n",
        "        print \"Alright, building the list of all the rhymes\"\n",
        "        for i in lyrics:\n",
        "            # grabs the last word in each bar\n",
        "            word = re.sub(r\"\\W+\", '', i.split(\" \")[-1]).lower()\n",
        "            # pronouncing.rhymes gives us a word that rhymes with the word being passed in\n",
        "            rhymeslist = pronouncing.rhymes(word)\n",
        "            # need to convert the unicode rhyme words to UTF8\n",
        "            rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "            # rhymeslistends contains the last two characters for each word\n",
        "            # that could potentially rhyme with our word\n",
        "            rhymeslistends = []\n",
        "            for i in rhymeslist:\n",
        "                rhymeslistends.append(i[-2:])\n",
        "            try:\n",
        "                # rhymescheme gets all the unique two letter endings and then\n",
        "                # finds the one that occurs the most\n",
        "                rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "            except Exception:\n",
        "                rhymescheme = word[-2:]\n",
        "            rhyme_master_list.append(rhymescheme)\n",
        "        # rhyme_master_list is a list of the two letters endings that appear\n",
        "        # the most in the rhyme list for the word\n",
        "        rhyme_master_list = list(set(rhyme_master_list))\n",
        "\n",
        "        reverselist = [x[::-1] for x in rhyme_master_list]\n",
        "        reverselist = sorted(reverselist)\n",
        "        # rhymelist is a list of the two letter endings (reversed)\n",
        "        # the reason the letters are reversed and sorted is so\n",
        "        # if the network messes up a little bit and doesn't return quite\n",
        "        # the right values, it can often lead to picking the rhyme ending next to the\n",
        "        # expected one in the list. But now the endings will be sorted and close together\n",
        "        # so if the network messes up, that's alright and as long as it's just close to the\n",
        "        # correct rhymes\n",
        "        rhymelist = [x[::-1] for x in reverselist]\n",
        "\n",
        "        #f = open(str(artist) + \".rhymes\", \"w\")\n",
        "        #f.write(\"\\n\".join(rhymelist))\n",
        "        #f.close()\n",
        "        print rhymelist\n",
        "        return rhymelist\n",
        "\n",
        "\n",
        "# converts the index of the most common rhyme ending\n",
        "# into a float\n",
        "def rhyme(line, rhyme_list):\n",
        "    word = re.sub(r\"\\W+\", '', line.split(\" \")[-1]).lower()\n",
        "    rhymeslist = pronouncing.rhymes(word)\n",
        "    rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "    rhymeslistends = []\n",
        "    for i in rhymeslist:\n",
        "        rhymeslistends.append(i[-2:])\n",
        "    try:\n",
        "        rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "    except Exception:\n",
        "        rhymescheme = word[-2:]\n",
        "    try:\n",
        "        float_rhyme = rhyme_list.index(rhymescheme)\n",
        "        float_rhyme = float_rhyme / float(len(rhyme_list))\n",
        "        return float_rhyme\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# grabs each line of the lyrics file and puts them\n",
        "# in their own index of a list, and then removes any empty lines\n",
        "# from the lyrics file and returns the list as bars\n",
        "def split_lyrics_file(text):\n",
        "    #text = open(text_file).read()\n",
        "    #text = text.split(\"\\n\")\n",
        "    while \"\" in text:\n",
        "        text.remove(\"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# only ran when not training\n",
        "def generate_lyrics(lyrics_file):\n",
        "    bars = []\n",
        "    last_words = []\n",
        "    lyriclength = len(lyrics_file)\n",
        "    count = 0\n",
        "    markov_model = markov((\". \").join(lyrics_file) + \".\")\n",
        "\n",
        "    while len(bars) < lyriclength / 9 and count < lyriclength * 2:\n",
        "        # By default, the make_sentence method tries, a maximum of 10 times per invocation,\n",
        "        # to make a sentence that doesn't overlap too much with the original text.\n",
        "        # If it is successful, the method returns the sentence as a string.\n",
        "        # If not, it returns None. (https://github.com/jsvine/markovify)\n",
        "        bar = markov_model.make_sentence()\n",
        "\n",
        "        # make sure the bar isn't 'None' and that the amount of\n",
        "        # syllables is under the max syllables\n",
        "        if type(bar) != type(None) and syllables(bar) < 1:\n",
        "\n",
        "            # function to get the last word of the bar\n",
        "            def get_last_word(bar):\n",
        "                last_word = bar.split(\" \")[-1]\n",
        "                # if the last word is punctuation, get the word before it\n",
        "                if last_word[-1] in \"!.?,\":\n",
        "                    last_word = last_word[:-1]\n",
        "                return last_word\n",
        "\n",
        "            last_word = get_last_word(bar)\n",
        "            # only use the bar if it is unique and the last_word\n",
        "            # has only been seen less than 3 times\n",
        "            if bar not in bars and last_words.count(last_word) < 3:\n",
        "                bars.append(bar)\n",
        "                last_words.append(last_word)\n",
        "                count += 1\n",
        "\n",
        "    return bars\n",
        "\n",
        "\n",
        "# used to construct the 2x2 inputs for the LSTMs\n",
        "# the lyrics being passed in are lyrics (original lyrics if being trained,\n",
        "# or ours if it's already trained)\n",
        "def build_dataset(lyrics, rhyme_list):\n",
        "    dataset = []\n",
        "    line_list = []\n",
        "    # line_list becomes a list of the line from the lyrics, the syllables for that line (either 0 or 1 since\n",
        "    # syllables uses integer division by maxsyllables (16)), and then rhyme returns the most common word\n",
        "    # endings of the words that could rhyme with the last word of line\n",
        "    for line in lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # using range(len(dataset)) - 3 because of the way the indices are accessed to\n",
        "    # get the lines\n",
        "    for i in range(len(dataset) - 3):\n",
        "        line1 = dataset[i][1:]\n",
        "        line2 = dataset[i + 1][1:]\n",
        "        line3 = dataset[i + 2][1:]\n",
        "        line4 = dataset[i + 3][1:]\n",
        "\n",
        "        # populate the training data\n",
        "        # grabs the syllables and rhyme index here\n",
        "        x = [line1[0], line1[1], line2[0], line2[1]]\n",
        "        x = np.array(x)\n",
        "        # the data is shaped as a 2x2 array where each row is a\n",
        "        # [syllable, rhyme_index] pair\n",
        "        x = x.reshape(2, 2)\n",
        "        x_data.append(x)\n",
        "\n",
        "        # populate the target data\n",
        "        y = [line3[0], line3[1], line4[0], line4[1]]\n",
        "        y = np.array(y)\n",
        "        y = y.reshape(2, 2)\n",
        "        y_data.append(y)\n",
        "\n",
        "    # returns the 2x2 arrays as datasets\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    # print \"x shape \" + str(x_data.shape)\n",
        "    # print \"y shape \" + str(y_data.shape)\n",
        "    return x_data, y_data\n",
        "\n",
        "# only used when not training\n",
        "def compose_rap(lines, rhyme_list, lyrics_file, model):\n",
        "    rap_vectors = []\n",
        "    human_lyrics = split_lyrics_file(lyrics_file)\n",
        "\n",
        "    # choose a random line to start in from given lyrics\n",
        "    initial_index = random.choice(range(len(human_lyrics) - 1))\n",
        "    # create an initial_lines list consisting of 2 lines\n",
        "    initial_lines = human_lyrics[initial_index:initial_index + 8]\n",
        "\n",
        "    starting_input = []\n",
        "    for line in initial_lines:\n",
        "        # appends a [syllable, rhyme_index] pair to starting_input\n",
        "        starting_input.append([syllables(line), rhyme(line, rhyme_list)])\n",
        "\n",
        "    # predict generates output predictions for the given samples\n",
        "    # it's reshaped as a (1, 2, 2) so that the model can predict each\n",
        "    # 2x2 matrix of [syllable, rhyme_index] pairs\n",
        "    starting_vectors = model.predict(np.array([starting_input]).flatten().reshape(4, 2, 2))\n",
        "    rap_vectors.append(starting_vectors)\n",
        "\n",
        "    for i in range(rap_length):\n",
        "        rap_vectors.append(model.predict(np.array([rap_vectors[-1]]).flatten().reshape(4, 2, 2)))\n",
        "\n",
        "    return rap_vectors\n",
        "\n",
        "\n",
        "def vectors_into_song(vectors, generated_lyrics, rhyme_list):\n",
        "    print \"\\n\\n\"\n",
        "    print \"About to write rap (this could take a moment)...\"\n",
        "    print \"\\n\\n\"\n",
        "\n",
        "    # compare the last words to see if they are the same, if they are\n",
        "    # increment a penalty variable which grants penalty points for being\n",
        "    # uncreative\n",
        "    def last_word_compare(rap, line2):\n",
        "        penalty = 0\n",
        "        for line1 in rap:\n",
        "            word1 = line1.split(\" \")[-1]\n",
        "            word2 = line2.split(\" \")[-1]\n",
        "\n",
        "            # remove any punctuation from the words\n",
        "            while word1[-1] in \"?!,. \":\n",
        "                word1 = word1[:-1]\n",
        "\n",
        "            while word2[-1] in \"?!,. \":\n",
        "                word2 = word2[:-1]\n",
        "\n",
        "            if word1 == word2:\n",
        "                penalty += 0.2\n",
        "\n",
        "        return penalty\n",
        "\n",
        "    # vector_half is a single [syllable, rhyme_index] pair\n",
        "    # returns a score rating for a given line\n",
        "    def calculate_score(vector_half, syllables, rhyme, penalty):\n",
        "        desired_syllables = vector_half[0]\n",
        "        desired_rhyme = vector_half[1]\n",
        "        # desired_syllables is the number of syllables we want\n",
        "        desired_syllables = desired_syllables * maxsyllables\n",
        "        # desired rhyme is the index of the rhyme we want\n",
        "        desired_rhyme = desired_rhyme * len(rhyme_list)\n",
        "\n",
        "        # generate a score by subtracting from 1 the sum of the difference between\n",
        "        # predicted syllables and generated syllables and the difference between\n",
        "        # the predicted rhyme and generated rhyme and then subtract the penalty\n",
        "        score = 1.0 - (abs((float(desired_syllables) - float(syllables))) + abs(\n",
        "            (float(desired_rhyme) - float(rhyme)))) - penalty\n",
        "\n",
        "        return score\n",
        "\n",
        "    # generated a list of all the lines from generated_lyrics with their\n",
        "    # line, syllables, and rhyme float value\n",
        "    dataset = []\n",
        "    for line in generated_lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    rap = []\n",
        "\n",
        "    vector_halves = []\n",
        "    for vector in vectors:\n",
        "        # vectors are the 2x2 rap_vectors (predicted bars) generated by compose_rap()\n",
        "        # separate every vector into a half (essentially one bar) where each\n",
        "        # has a pair of [syllables, rhyme_index]\n",
        "        vector_halves.append(list(vector[0][0]))\n",
        "        vector_halves.append(list(vector[0][1]))\n",
        "\n",
        "    for vector in vector_halves:\n",
        "        # Each vector (predicted bars) is scored against every generated bar ('item' below)\n",
        "        # to find the generated bar that best matches (highest score) the vector predicted\n",
        "        # by the model. This bar is then added to the final rap and also removed from the\n",
        "        # generated lyrics (dataset) so that we don't get duplicate lines in the final rap.\n",
        "        scorelist = []\n",
        "        for item in dataset:\n",
        "            # item is one of the generated bars from the Markov model\n",
        "            line = item[0]\n",
        "\n",
        "            if len(rap) != 0:\n",
        "                penalty = last_word_compare(rap, line)\n",
        "            else:\n",
        "                penalty = 0\n",
        "            # calculate the score of the current line\n",
        "            total_score = calculate_score(vector, item[1], item[2], penalty)\n",
        "            score_entry = [line, total_score]\n",
        "            # add the score of the current line to a scorelist\n",
        "            scorelist.append(score_entry)\n",
        "\n",
        "        fixed_score_list = []\n",
        "        for score in scorelist:\n",
        "            fixed_score_list.append(float(score[1]))\n",
        "        # get the line with the max valued score from the fixed_score_list\n",
        "        max_score = max(fixed_score_list)\n",
        "        for item in scorelist:\n",
        "            if item[1] == max_score:\n",
        "                # append item[0] (the line) to the rap\n",
        "                rap.append(item[0])\n",
        "                print str(item[0])\n",
        "\n",
        "                # remove the line we added to the rap so\n",
        "                # it doesn't get chosen again\n",
        "                for i in dataset:\n",
        "                    if item[0] == i[0]:\n",
        "                        dataset.remove(i)\n",
        "                        break\n",
        "                break\n",
        "    return rap\n",
        "\n",
        "\n",
        "def train(x_data, y_data, model):\n",
        "    # fit is used to train the model for 5 'epochs' (iterations) where\n",
        "    # the x_data is the training data, and the y_data is the target data\n",
        "    # x is the training and y is the target data\n",
        "    # batch_size is a subset of the training data (2 in this case)\n",
        "    # verbose simply shows a progress bar\n",
        "    model.fit(np.array(x_data), np.array(y_data),\n",
        "              batch_size=4,\n",
        "              epochs=epochs_to_train,\n",
        "              verbose=1)\n",
        "    # save_weights saves the best weights from training to a hdf5 file\n",
        "    #model.save_weights(artist + \".rap\")\n",
        "\n",
        "\n",
        "def main(depth):\n",
        "    train_mode = True\n",
        "    model = create_network(depth)\n",
        "    # change the lyrics file to the file with the lyrics you want to be trained on\n",
        "    \n",
        "    text_file = rap_source\n",
        "    \n",
        "    bars = split_lyrics_file(text_file)\n",
        "\n",
        "\n",
        "\n",
        "    rhyme_list = rhymeindex(bars)\n",
        "    \n",
        "    x_data, y_data = build_dataset(bars, rhyme_list)\n",
        "    train(x_data, y_data, model)\n",
        "    \n",
        "    bars = generate_lyrics(text_file)\n",
        "    \n",
        "    vectors = compose_rap(bars, rhyme_list, text_file, model)\n",
        "    rap = vectors_into_song(vectors, bars, rhyme_list)\n",
        "    \n",
        "    for bar in rap:\n",
        "        print bar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ln_WuWhryOj",
        "colab_type": "text"
      },
      "source": [
        "Run this chunk of code below to call the main function and kick the rap generation off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXx1lJ9H-tPr",
        "colab_type": "code",
        "outputId": "7603f2f9-669b-4939-d038-2d260682703e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1611
        }
      },
      "source": [
        "main(depth) # run this to get the actual rap"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e316dd91dea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run this to get the actual rap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-54baabe65edb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(depth)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mtrain_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0;31m# change the lyrics file to the file with the lyrics you want to be trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-54baabe65edb>\u001b[0m in \u001b[0;36mcreate_network\u001b[0;34m(depth)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 4 units (nodes), and a 2x2 tensor (which is the same shape as the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# training data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# adds 'depth' number of layers to the network with 8 nodes each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    591\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1797\u001b[0m                                       \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m   1800\u001b[0m         self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   1801\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                             constraint=constraint)\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    257\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;34m\"construct, such as a loop or conditional. When creating a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;34m\"variable inside a loop or conditional, use a lambda as the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 \"initializer.\" % name)\n\u001b[0m\u001b[1;32m    388\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           shape = (self._initial_value.get_shape()\n",
            "\u001b[0;31mValueError\u001b[0m: Initializer for variable lstm_8/kernel/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer."
          ]
        }
      ]
    }
  ]
}